{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-22\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Handle Dataframe\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-AMLD2O7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handle Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20b28bec1a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"Datasets/testfile2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|     Yash|  23|         1|  8000|\n",
      "|    Mohan|  25|         4| 25000|\n",
      "|Sudhanshu|  30|         8| 35000|\n",
      "|   Mahesh|NULL|         6| 32000|\n",
      "|    Krish|  21|      NULL|  6000|\n",
      "|    Harsh|  42|        16| 65000|\n",
      "|  Shubham|  56|        23| 77000|\n",
      "|     NULL|  26|      NULL|  NULL|\n",
      "|     NULL|NULL|         6| 56000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|     Yash| 23|         1|  8000|\n",
      "|    Mohan| 25|         4| 25000|\n",
      "|Sudhanshu| 30|         8| 35000|\n",
      "|    Harsh| 42|        16| 65000|\n",
      "|  Shubham| 56|        23| 77000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop nan or null value whole row\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|     Yash|  23|         1|  8000|\n",
      "|    Mohan|  25|         4| 25000|\n",
      "|Sudhanshu|  30|         8| 35000|\n",
      "|   Mahesh|NULL|         6| 32000|\n",
      "|    Krish|  21|      NULL|  6000|\n",
      "|    Harsh|  42|        16| 65000|\n",
      "|  Shubham|  56|        23| 77000|\n",
      "|     NULL|  26|      NULL|  NULL|\n",
      "|     NULL|NULL|         6| 56000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any--how in this it will only drop if in a row we have all the values are null\n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|     Yash| 23|         1|  8000|\n",
      "|    Mohan| 25|         4| 25000|\n",
      "|Sudhanshu| 30|         8| 35000|\n",
      "|    Harsh| 42|        16| 65000|\n",
      "|  Shubham| 56|        23| 77000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any--how in this it will drop the rows even if one null value it is bydefault in normal drop\n",
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|     Yash|  23|         1|  8000|\n",
      "|    Mohan|  25|         4| 25000|\n",
      "|Sudhanshu|  30|         8| 35000|\n",
      "|   Mahesh|NULL|         6| 32000|\n",
      "|    Krish|  21|      NULL|  6000|\n",
      "|    Harsh|  42|        16| 65000|\n",
      "|  Shubham|  56|        23| 77000|\n",
      "|     NULL|NULL|         6| 56000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold for eg 2 then it will not delete the row where with null value atleast 2 non null values also there\n",
    "# it will check wherever is null value row there should be 2 non null values should be there if not then it will be deleted\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|     Yash|  23|         1|  8000|\n",
      "|    Mohan|  25|         4| 25000|\n",
      "|Sudhanshu|  30|         8| 35000|\n",
      "|   Mahesh|NULL|         6| 32000|\n",
      "|    Krish|  21|      NULL|  6000|\n",
      "|    Harsh|  42|        16| 65000|\n",
      "|  Shubham|  56|        23| 77000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold for eg 3 then it will not delete the row where with null value atleast 2 non null values also there\n",
    "# it will check wherever is null value row there should be 3 non null values should be there if not then it will be deleted\n",
    "df_pyspark.na.drop(how=\"any\", thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|     Yash|  23|         1|  8000|\n",
      "|    Mohan|  25|         4| 25000|\n",
      "|Sudhanshu|  30|         8| 35000|\n",
      "|   Mahesh|NULL|         6| 32000|\n",
      "|    Harsh|  42|        16| 65000|\n",
      "|  Shubham|  56|        23| 77000|\n",
      "|     NULL|NULL|         6| 56000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset it will delete null value rows from only selected column\n",
    "df_pyspark.na.drop(how=\"any\", subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|     Yash| 23|         1|  8000|\n",
      "|    Mohan| 25|         4| 25000|\n",
      "|Sudhanshu| 30|         8| 35000|\n",
      "|    Krish| 21|      NULL|  6000|\n",
      "|    Harsh| 42|        16| 65000|\n",
      "|  Shubham| 56|        23| 77000|\n",
      "|     NULL| 26|      NULL|  NULL|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset it will delete null value rows from only selected column\n",
    "df_pyspark.na.drop(how=\"any\", subset=[\"Age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|         Yash|  23|         1|  8000|\n",
      "|        Mohan|  25|         4| 25000|\n",
      "|    Sudhanshu|  30|         8| 35000|\n",
      "|       Mahesh|NULL|         6| 32000|\n",
      "|        Krish|  21|      NULL|  6000|\n",
      "|        Harsh|  42|        16| 65000|\n",
      "|      Shubham|  56|        23| 77000|\n",
      "|Missing Value|  26|      NULL|  NULL|\n",
      "|Missing Value|NULL|         6| 56000|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## filling missing value\n",
    "df_pyspark.na.fill(\"Missing Value\").show()\n",
    "# so the interger value was not filled only the strings were filled\n",
    "# to fill integer values also we have to inferSchema=False then all the data will be taken as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"Datasets/testfile2.csv\", header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|         Yash|           23|            1|         8000|\n",
      "|        Mohan|           25|            4|        25000|\n",
      "|    Sudhanshu|           30|            8|        35000|\n",
      "|       Mahesh|Missing Value|            6|        32000|\n",
      "|        Krish|           21|Missing Value|         6000|\n",
      "|        Harsh|           42|           16|        65000|\n",
      "|      Shubham|           56|           23|        77000|\n",
      "|Missing Value|           26|Missing Value|Missing Value|\n",
      "|Missing Value|Missing Value|            6|        56000|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(\"Missing Value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Yash|  23|         1|  8000|         23|                 1|          8000|\n",
      "|    Mohan|  25|         4| 25000|         25|                 4|         25000|\n",
      "|Sudhanshu|  30|         8| 35000|         30|                 8|         35000|\n",
      "|   Mahesh|NULL|         6| 32000|         31|                 6|         32000|\n",
      "|    Krish|  21|      NULL|  6000|         21|                 9|          6000|\n",
      "|    Harsh|  42|        16| 65000|         42|                16|         65000|\n",
      "|  Shubham|  56|        23| 77000|         56|                23|         77000|\n",
      "|     NULL|  26|      NULL|  NULL|         26|                 9|         38000|\n",
      "|     NULL|NULL|         6| 56000|         31|                 6|         56000|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filing null values by mean you can choose median or mode also but for this infer=True is must to take numeric value as integer and not as a string\n",
    "\n",
    "df_pyspark = spark.read.csv(\"Datasets/testfile2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "imputer = Imputer(inputCols=[\"Age\", \"Experience\", \"Salary\"], outputCols=[\"{}_imputed\".format(c) for c in [\"Age\", \"Experience\", \"Salary\"]]).setStrategy(\"mean\")\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
